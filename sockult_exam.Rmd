---
title: "SocCult experiment"
output: html_document
---

#Load and clean
```{r clean up}  

#clean up the data

clean_up= function(file_name) {
  #read in data and replace the empty places with NA
  clean= read.csv(file_name, na.strings=c(""," ","NA"))
  
  #add which condition it was
  g= basename(file_name)
  f=unlist(strsplit(g, "[.]"))
  clean$condition=f[1]
  clean$condition=as.factor(clean$condition)
  
  #code from the internet - just adds a number to every row
  id <- seq.int(nrow(clean))
  
  #create unique ids
  clean$id= paste(clean$condition, id)
  
  #get out and rename the needed variables
  clean_new= data.frame(id= clean$id,
                        condition=clean$condition,
                        age=round(clean$How.old.are.you.,0),
                        gender=clean$What.is.your.gender.,
                        country=clean$Where.do.you.live.,
                        willingness_dkk=clean$You.are.alone.in.a.room.and.have.100.DKK.on.a.table.in.front.of.you..You.can.give.to.charity.as.much.as.you.want..the.amount.can.be.as.low.as.1.DKK...You.can.keep.the.rest..Would.you.donate.to.charity.,
                        amount_dkk=clean$You.are.alone.in.a.room.and.have.100.DKK.on.a.table.in.front.of.you..You.must.donate.some.part.of.it..You.can.keep.the.rest..Please.indicate.how.much.you.would.DONATE.,
                        willingness_huf=clean$You.are.alone.in.a.room.and.have.4000.HUF.on.a.table.in.front.of.you..You.can.give.to.charity.as.much.as.you.want..the.amount.can.be.as.low.as.1.HUF...You.can.keep.the.rest..Would.you.donate.to.charity.,
                        amount_huf=clean$You.are.alone.in.a.room.and.have.4000.HUF.on.a.table.in.front.of.you..You.must.donate.some.part.of.it..You.can.keep.the.rest..Please.indicate.how.much.you.would.DONATE.,
                        feel_before=clean$Please.indicate.how.you.feel.now,
                        feel_after=clean$Please.indicate.how.you.feel.now.1,
                        trust_in_donation=clean$To.what.extent.do.you.trust.that.donations.in.real.life.arrive.to.where.they.should.,
                        notice= clean$Did.you.notice.the.picture.on.top.before.seeing.this.question.,
                        alone= clean$How.many.people.can.you.see.if.you.look.around...Don.t.look.out.the.windows..,
                        comp_or_phone= clean$Are.you.filling.in.this.questionnaire.on.a.phone.or.computer.
  )
  
  #from the internet https://stackoverflow.com/questions/18115550/how-to-combine-two-or-more-columns-in-a-dataframe-into-a-new-column-with-a-new-n?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa
  
  #function to merge the 4 columns I have about donation
  #creates 2 columns, gets rid of NAs
  paste_noNA <- function(x,sep=", ")
    gsub(", " ,sep, toString(x[!is.na(x) & x!="" & x!="NA"] ) )
  
  #one column with all the info separated by comma
  #no NAs
  sep=" , "
  clean_new$n <- apply( clean_new[ , c(6:9) ] , 1 , paste_noNA , sep=sep)
  
  #separates the info into the two columns it has to be in
  clean_new$willingness=stringr::str_split_fixed(clean_new$n, " , ", 2) [,1]
  clean_new$amount_r=stringr::str_split_fixed(clean_new$n, " , ", 2) [,2]
  
  #get rid of unnecessary columns
  clean_data= clean_new[,-c(6:9, 16)]
  
  #correct the variables
  
  clean_data$amount_r=as.factor(clean_data$amount_r)
  
  #add extra variable for amount to express which is bigger
  clean_data$amount_f=plyr::revalue(clean_data$amount_r, c("donate less than 10 DKK"="1", "donate 10-30 DKK"= "2","donate 30-50 DKK"="3", "donate 50-70 DKK"="4", "donate 70-90 DKK"="5", "donate 100 DKK (donate all the money)"="6", "donate less than 400 HUF"="1", "donate 400-1200 HUF"="2", "donate 1200-2000 HUF"= "3", "donate 2000-2800 HUF"="4", "donate 2800-3600 HUF"="5", "donate 4000 HUF (donate all the money)"= "6"))
  
  clean_data$amount_f=factor(clean_data$amount_f, levels=c("1","2","3","4","5","6"))
  
  clean_data$amount_n= as.numeric(as.character(clean_data$amount_f))

  #willingness
  clean_data$willingness_f=factor(plyr::revalue(clean_data$willingness, c("Yes" = "1", "No"="0")))
  clean_data$willingness_n= as.numeric(as.character(clean_data$willingness_f))
  
  #add extra variable for whether the eyes were present
  clean_data$eyes_f= plyr::revalue(clean_data$condition, c("Happy"= "1", "Neutral"= "1", "Control"= "0"))
  clean_data$eyes_f=factor(clean_data$eyes_f, levels=c("0", "1"))
  clean_data$eyes_n= as.numeric(as.character(clean_data$eyes_f))

  
  clean_data$emotion_f= plyr::revalue(clean_data$condition, c("Happy"= "1", "Neutral"= "0", "Control"= NA))
  clean_data$emotion_f=factor(clean_data$emotion_f, levels=c("0", "1"))
   clean_data$emotion_n= as.numeric(as.character(clean_data$emotion_f))
 
   clean_data$alone_f= plyr::revalue(clean_data$alone, c("I'm alone"= "1", "less than 5"= "0", "more than 5"= "0"))
   clean_data$alone_f=factor(clean_data$alone_f, levels=c("0", "1"))
   clean_data$alone_n= as.numeric(as.character(clean_data$alone_f))
   
   clean_data$size_f= plyr::revalue(clean_data$comp_or_phone, c("Computer"= "0", "Phone"= "1", "Other"= "0"))
   clean_data$size_f=factor(clean_data$size_f, levels=c("0", "1"))
   clean_data$size_n= as.numeric(as.character(clean_data$size_f))
   
    clean_data$notice_f= plyr::revalue(clean_data$notice, c("Yes"= "1", "No"= "0"))
  clean_data$notice_f=factor(clean_data$notice_f, levels=c("0", "1"))
   clean_data$notice_n= as.numeric(as.character(clean_data$notice_f))
   
   
  return(clean_data)
}

# #might need later
#   #to count the characters of the file path+name:  nchar()
#   #clean_da$condition= substr(file_name, 52,80)
```


```{r Read in cleaned up data and merge}  

control_data=clean_up("C:/Users/torda/Documents/egyetem/sockult/eyes/data/Control.csv")
happy_data=clean_up("C:/Users/torda/Documents/egyetem/sockult/eyes/data/Happy.csv")

neutral_data=clean_up("C:/Users/torda/Documents/egyetem/sockult/eyes/data/Neutral.csv")

a=rbind(happy_data, neutral_data)

all_data=rbind(a, control_data)
rm(a, control_data, happy_data, neutral_data, clean_up)

emotion_data= na.omit(all_data)

#if I save all_data and read it in again the variable types are messed with, so keep it like this
```



#Look at the data
```{r plots}


library(ggplot2)

#look at 
#whether people seeing face depends from using comp/phone
#for both, they tend to notice the stimuli more often than not notice it.
#they notice it more in the computer condition

ggplot(all_data) +
  geom_bar(aes(x= comp_or_phone)) +
  facet_grid(~ notice)

#whether people seeing face depends from which condition
#people tend to notice more when it's a face than when it's two apples

ggplot(all_data) +
  geom_bar(aes(x= condition)) +
  facet_grid(~ notice)

#whether there's any diff in feel before and after (by willingness and amount?)
#no, for most people no change - sure?? I'm not sure how to interpret it, also it's a bit weird that it looks like it's continous even if it's treated as factors?
ggplot(all_data) +
  geom_jitter(aes(x= (as.factor(feel_before)), y=as.factor(feel_after)))

#how to read these...?
ggplot(all_data) +
  geom_bar(aes(x= (as.factor(feel_before)), fill=(willingness_f)))

ggplot(all_data) +
  geom_bar(aes(x= (as.factor(feel_after)), fill=(willingness_f)))

#gets hard to interpret, but maybe I get these more? 
#probably can tell from these that given somebody donated or not how many of them felt better/ the same
ggplot(all_data) +
  geom_bar(aes(x= (as.factor(feel_before)), fill=(as.factor(feel_after)))) +
  facet_grid(~ willingness_f)

#... what's this? should be the same as before, only with the amount of donated money - given you donated x, do you feel better/ the same?
ggplot(all_data) +
  geom_bar(aes(x= (as.factor(feel_before)), fill=(as.factor(feel_after)))) +
  facet_grid(~ amount_f)

#plot willingness and amount by condition
#after conditions will be balanced, from this I can see whether willingness to donate depends on condition
ggplot(all_data) +
  geom_bar(aes(x= (condition), fill=(willingness_f))) 


ggplot(all_data) +
  geom_bar(aes(x= (amount_f), fill=(condition))) 

#does willingness or amount depend on seeing eyes?
#what do these mean...?
ggplot(all_data) +
  geom_bar(aes(x= (willingness_f), fill= eyes_f)) 

ggplot(all_data) +
  geom_bar(aes(x= (amount_f), fill= eyes_f))

#does willingness or amount depend on watching eyes?
#nope

ggplot(emotion_data) +
  geom_bar(aes(x= (willingness_f), fill= emotion_f)) 

ggplot(emotion_data) +
  geom_bar(aes(x= (amount_f), fill= emotion_f)) 
```


#Models
based on Nettle et al.'s paper
work from all_data, all three conditions

hypothesis 1:
Watching eyes make people more willing to donate.
willingness ~ eyes


hypothesis 2:
Watching eyes make no difference for the amount of donation.
amount ~ eyes

adding emotion
work from emotion_data, control excluded

hypothesis 3:
Happy face (eyes) make people more willing to donate.
willingness ~ emotion

hypothesis 4:
Happy face (eyes) make people donate bigger amount of money.
amount ~ emotion

```{r logistic regression models - willingness, eyes}

library(rethinking)

#just alpha
m_wiley_alpha <- map2stan(
  alist(
    willingness_n ~ dbinom( 1 , p ) ,
    logit(p) <- a,
    a ~ dnorm(0,10)
  ) ,
  data=all_data,
  chains=2 , cores=2, iter = 2000)


#check if the chains are doing alright
plot(m_wiley_alpha)

#results
# precis(m_wiley_a)
# logistic(0.84) #there was a 0.70 preference for donating
# logistic(c(0.58, 1.06)) #0.6410674 0.7426905


#just eyes
m_wiley_e <- map2stan(
  alist(
    willingness_n ~ dbinom( 1 , p ) ,
    logit(p) <- a + be*eyes_n,
    a ~ dnorm(0,10),
    be ~ dnorm(0,10)
  ) ,
  data=all_data,
  chains=2 , cores=2, iter = 2000)


#check if the chains are doing alright
plot(m_wiley_e)


#eyes and alone
m_wiley_ea <- map2stan(
  alist(
    willingness_n ~ dbinom( 1 , p ) ,
    logit(p) <- a + be*eyes_n + ba*alone_n,
    a ~ dnorm(0,10),
    be ~ dnorm(0,10),
    ba~ dnorm(0, 10)
  ) ,
  data=all_data,
  chains=2 , cores=2, iter = 2000)


#check if the chains are doing alright
plot(m_wiley_ea)

#eyes and alone int (eyes are only affective when not alone?)
m_wiley_eai <- map2stan(
  alist(
    willingness_n ~ dbinom( 1 , p ) ,
    logit(p) <- a + (be + ba*alone_n)*eyes_n,
    a ~ dnorm(0,10),
    be ~ dnorm(0,10),
    ba~ dnorm(0, 10)
  ) ,
  data=all_data,
  chains=2 , cores=2, iter = 2000)

#eyes and size int (eyes are only affective when screen is big enough?)
m_wiley_es <- map2stan(
  alist(
    willingness_n ~ dbinom( 1 , p ) ,
    logit(p) <- a + (be + bs*size_n)*eyes_n,
    a ~ dnorm(0,10),
    be ~ dnorm(0,10),
    bs~ dnorm(0, 10)
  ) ,
  data=all_data,
  chains=2 , cores=2, iter = 2000)

#eyes and notice int (eyes only affective when noticed?)
m_wiley_en <- map2stan(
  alist(
    willingness_n ~ dbinom( 1 , p ) ,
    logit(p) <- a + (be + bn*notice_n)*eyes_n,
    a ~ dnorm(0,10),
    be ~ dnorm(0,10),
    bn~ dnorm(0, 10)
    ) ,
  data=all_data,
  chains=2 , cores=2, iter = 2000)

#which one is best?
compare(m_wiley_alpha, m_wiley_e, m_wiley_ea, m_wiley_eai, m_wiley_es, m_wiley_en)


```

```{r extra}
#does noticing depend on screen size?
m_notice_size <- map2stan(
  alist(
    notice_n ~ dbinom( 1 , p ) ,
    logit(p) <- a + bs*size_n,
    a ~ dnorm(0,10),
    bs ~ dnorm(0,10)
    ) ,
  data=all_data,
  chains=2 , cores=2, iter = 2000)

#size: 0 is for computer and other (big), 1 is for phone (small)
precis(m_notice_size)
logistic(-0.47) #0.38 small size decreases the chance of noticing 
exp(-0.47) #0.62 there is a 62% proportional decrease in the odds of noticing when the screen is small
```


```{r}
small= all_data[1:7,]

# dummy data for predictions across treatments 
d.pred <- data.frame(eyes_n = c(0,1,0,1), #no, yes, no, yes
                     alone_n = c(0,0,1,1)) # no, no, yes, yes
# build prediction ensemble 
wil.ensemble <- ensemble( m_wiley_alpha , m_wiley_e , m_wiley_ea , data=small )
# summarize 
pred.p <- apply( wil.ensemble$link , 2 , mean ) 
pred.p.PI <- apply( wil.ensemble$link , 2 , PI )

# empty plot frame with good axes 
plot( 0 , 0 , type="n" , xlab="eyes/alone" ,
      ylab="proportion eyes" , ylim=c(0,1) , xaxt="n" ,
      xlim=c(1,4) ) 
axis( 1 , at=1:4 , labels=c("0/0","1/0","0/1","1/1") )
# plot raw data, one trend for each of 7 individual chimpanzees 
#all_data$id=as.numeric(all_data$id)
p <- by( small$willingness_n , 
         list(small$eyes_n,small$alone_n,small$id), mean ) 
for ( participant in 1:7 ) 
  lines( 1:4 , as.vector(p[,,participant]) , col=rangi2 , lwd=1.5 )
# now superimpose posterior predictions 
lines( 1:4 , pred.p ) 
shade( pred.p.PI , 1:4 )


```


```{r logistic regression models - willingness, emotion}

```



#Other
```{r}


#based on Nettle et al.'s paper

# willingness ~ eyes
#alpha only - willingness to donate
model1 <- map( alist( 
   willingness ~ dbinom( 1 , p ) , 
   logit(p) <- a , 
   a ~ dnorm(0,10)
   ) , data=all_data )

precis(model1)

# a+ b*eyes - 
model2 <- map( alist( 
   willingness ~ dbinom( 1 , p ) , 
   logit(p) <- a + b*eyes, 
   a ~ dnorm(0,10),
   b~dnorm(0,10)
   ) , data=all_data )

precis(model2)

compare(model1, model2)

# amount ~ eyes

model6 <- map( 
  alist( 
    amount ~ dordlogit( phi , c(a1,a2,a3,a4,a5,a6) ), 
    phi <- 0, 
    c(a1,a2,a3,a4,a5,a6) ~ dnorm(0,10)) , 
  data=all_data , start=list(a1=1,a2=2,a3=3,a4=4,a5=5,a6=6) )

precis(model6)
logistic(coef(model6))

model7 <- map( 
  alist( amount ~ dordlogit( phi , c(a1,a2,a3,a4,a5,a6) ) , 
         phi <- bA*willingness_f,
         c(bA) ~ dnorm(0,10), 
         c(a1,a2,a3,a4,a5,a6) ~ dnorm(0,10)
         ) , 
  data=all_data , start=list(a1=1,a2=2,a3=3,a4=4,a5=5,a6=6) )

coeftab(model6, model7)
#what's wrong?
compare(model6, model7)

#adding emotion

# willingness ~ emotion
model3 <- map( alist( 
   willingness ~ dbinom( 1 , p ) , 
   logit(p) <- a , 
   a ~ dnorm(0,10)
   ) , data=emotion_data )

precis(model3)

# a+ b*eyes - 
model4 <- map( alist( 
   willingness ~ dbinom( 1 , p ) , 
   logit(p) <- a + b*emotion, 
   a ~ dnorm(0,10),
   b~dnorm(0,10)
   ) , data=emotion_data )

precis(model4)

compare(model3, model4)


# amount ~ emotion


```

```{r}
model5 <- map2stan( model4 , data=emotion_data , iter=1e4 , cores= 1, chains= 2, warmup=1000 ) 
precis(model5)

#check if the chains are doing alright
plot(model5)

#plot results
pairs(model5)

```

```{r from Riccardo!}
all_data %>% group_by(willingness, emotion) %>% summarise(mean= mean(amount_n)) 
all_data %>% group_by(willingness) %>% summarise(mean= mean(amount_n))
all_data %>% group_by(emotion) %>% summarise(mean= mean(amount_n))
 all_data %>% group_by(eyes) %>% summarise(mean= mean(amount_n))
 
 all_data %>% group_by(eyes_f) %>% summarise(propensity= sum(willingness) / n())
```

> x=extract.samples(model4)
> sum(x$b > 0)
[1] 7293
> sum(x$b > 0) / 10000
[1] 0.7293
